{"cells":[{"metadata":{"id":"rIARxjF103ay"},"cell_type":"markdown","source":"# ***Data Analysis***\n#***YAHOO Finance Stock dataset Statistical analysis - Laptev Oleg, gr.4167, RCSE-GRIAT***\n\n> Working with CSV format Yahoo Finance Inc. dataset made for the dash-demo application\n\n> Using: *Matplotlib,Pandas,Statsmodels API,ARIMA,ANOVA,K-MEANS,Regression and Classification models, Clustering*\n\n\n\n"},{"metadata":{"id":"f1HmZy632ABP"},"cell_type":"markdown","source":"**Preparing the data**"},{"metadata":{"id":"leHUt9k32d1P","outputId":"c07f7390-80cd-4893-948e-f9906ec8b876","trusted":false},"cell_type":"code","source":"!pip3 install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"id":"-JsxHhYZ0srW","trusted":false},"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom pmdarima.arima import auto_arima\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math","execution_count":null,"outputs":[]},{"metadata":{"id":"XUjpN2VrReyv","outputId":"42db83a3-bfcb-449c-9287-03e2e4ada1b3","trusted":false},"cell_type":"code","source":"pip install yfinance","execution_count":null,"outputs":[]},{"metadata":{"id":"jRY94zwWhmwW","outputId":"1c21f765-5eea-49be-caeb-27dad96bae0a","trusted":false},"cell_type":"code","source":"df = pd.read_csv(r'/content/drive/My Drive/prices.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"k68dWAVGhZeD","outputId":"bd12b961-a354-4b2f-9c85-7e8bb64ab520","trusted":false},"cell_type":"code","source":"df['date']","execution_count":null,"outputs":[]},{"metadata":{"id":"OjP_85n_hh18","outputId":"9cb51bf3-7329-4b90-d10b-c2f23e3fc484","trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"1BG8RKgH4JGU","trusted":false},"cell_type":"code","source":"from datetime import datetime\ncon=df['date']\ndf['date']=pd.to_datetime(df['date'])\ndf.set_index('date', inplace=True)\n#check datatype of index\n","execution_count":null,"outputs":[]},{"metadata":{"id":"x8dHPVTDI9S1","trusted":false},"cell_type":"code","source":"df=df.sort_values(by='date')","execution_count":null,"outputs":[]},{"metadata":{"id":"m5gExHKuiH8M","outputId":"889a19b5-ffe0-474f-f4d8-d1b2ad2d3741","trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"VN4SyebVd8fz","trusted":false},"cell_type":"code","source":"df1=df.loc[df['symbol']=='WMT']\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JnqHLkODihxR","outputId":"66b686ae-accc-43ae-a1bf-0aa6fafe9025","trusted":false},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"id":"nFVu2tc0gBGU","trusted":false},"cell_type":"code","source":"dfopen=df1['open']\n","execution_count":null,"outputs":[]},{"metadata":{"id":"F6ZFwvaV4P8A","trusted":false},"cell_type":"code","source":"\ndf1['year'] = df1.index.year\ndf1['month'] = df1.index.month\ndf1['day'] = df1.index.day","execution_count":null,"outputs":[]},{"metadata":{"id":"UPExV-ld4gVQ"},"cell_type":"markdown","source":"\n\n> Display a random sampling of 5 rows\n\n"},{"metadata":{"id":"f1QyFdSG4YpE","outputId":"4b08e8ef-3c17-4358-fc42-53c66da0aa73","trusted":false},"cell_type":"code","source":"\ndf1.sample(5, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"MTkBgsJFkhwF","trusted":false},"cell_type":"code","source":"df.columns = df.columns.get_level_values(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"7PfxzP_TkjcW","outputId":"454cf489-d078-4648-9672-e1de95ce1fac","trusted":false},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"wg9x9y0Rko5b","trusted":false},"cell_type":"code","source":"df.columns = [' '.join(col).strip() for col in df.columns.values]","execution_count":null,"outputs":[]},{"metadata":{"id":"GtbAPjOykzXQ","outputId":"49df5a11-9677-4e42-df90-822b8ee53519","trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"f7jgydcdk3pf","outputId":"024e7a37-5dd1-4d37-91c9-2d9b549872ad","trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"jXzgMCgP4oo1","outputId":"62e175cc-2f57-4bfe-e41a-1a3387bd993b","trusted":false},"cell_type":"code","source":"temp=df1.groupby(['date'])['open'].mean() \ntemp.plot(figsize=(15,5), title= 'Opening Prices(Monthwise)', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"PJgstMYo4wql","outputId":"b21502c9-8c15-4772-ea29-14ffa60c7165","trusted":false},"cell_type":"code","source":"df1.groupby('month')['open'].mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"id":"h_mhnWAGi9cb","trusted":false},"cell_type":"code","source":"dfopen=df1['open'].loc[:'20140530']","execution_count":null,"outputs":[]},{"metadata":{"id":"ed870mEDAWHv","outputId":"36033bd7-05de-4637-c93b-fd89cb082185","trusted":false},"cell_type":"code","source":"histogram=dfopen.describe()\ndfopen.hist()\nhistogram","execution_count":null,"outputs":[]},{"metadata":{"id":"ZuqX5MT645mL","outputId":"0fcf3296-98fb-4437-9557-9b1bfea57814","trusted":false},"cell_type":"code","source":"test = dfopen[1030:]\ntrain = dfopen[:1029]\ntrain.plot(style='b-')\ntest.plot(style='r--')","execution_count":null,"outputs":[]},{"metadata":{"id":"0x1-IdtTKiKW"},"cell_type":"markdown","source":""},{"metadata":{"id":"3M-by_cD4_su","trusted":false},"cell_type":"code","source":"def test_stationarity(timeseries):\n #Determing rolling statistics\n rolmean = timeseries.rolling(12).mean()\n rolstd = timeseries.rolling(12).std()\n #Plot rolling statistics:\n plt.plot(timeseries, color='blue',label='Original')\n plt.plot(rolmean, color='orange', label='Rolling mean')\n plt.plot(rolstd, color='black', label = 'Rolling std')\n plt.legend(loc='Best')\n plt.title('Rolling Mean and Standard Deviation')\n plt.show(block=False)\n \n print(\"Results of Dickey-Fuller \")\n adftest = adfuller(timeseries,autolag='AIC')\n # output for adft will give us without defining what the values are.\n #hence we manually write what values does it explain using a for loop\n output = pd.Series(adftest[0:4],index=['Test stat.','p-value','Num. of lags used','Num of observations used'])\n for key,values in adftest[4].items():\n  output['critical value (%s)'%key] = values\n print(output)","execution_count":null,"outputs":[]},{"metadata":{"id":"N1K3YX3e5PY3"},"cell_type":"markdown","source":"\n\n> Test the data by algorithm\n\n"},{"metadata":{"id":"-Mu0bHSY5Z1F","outputId":"afed3835-6d94-4a84-93d3-1f8f1d486f30","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,14))\n\ntest_stationarity(train)","execution_count":null,"outputs":[]},{"metadata":{"id":"axCFV4qk7Sof"},"cell_type":"markdown","source":"**Critical value, the p-value is greater than 5%, and we can see an increasing trend in the data. It means that data is not stationary.**\n> *Hence, we have to stationarize it*\n\n"},{"metadata":{"id":"FaupywH77d9R","outputId":"a2ba7cc0-187d-4c18-9bdc-fd64424bf4e9","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,14))\n\ntrain_log = np.log(train) \ntest_log = np.log(test)\nmoving_avg = train_log.rolling(24).mean() \nplt.plot(train_log) \nplt.plot(moving_avg, color = 'red') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"RVmQSfBz73tm"},"cell_type":"markdown","source":"\n\n> We can observe that there is a trend, so it is not stationary. Now we will remove this trend to make our time series stationary.\n\n"},{"metadata":{"id":"y9PTul48_AIa","trusted":false},"cell_type":"code","source":"train_log_moving_average_diff = train_log - moving_avg","execution_count":null,"outputs":[]},{"metadata":{"id":"X2-na383_WYS","outputId":"4fa92361-1bd7-44e3-f4a2-96cec421c07e","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,14))\n\ntrain_log_moving_average_diff.dropna(inplace = True), test_stationarity(train_log_moving_average_diff)","execution_count":null,"outputs":[]},{"metadata":{"id":"3zyP1TEvBQnc"},"cell_type":"markdown","source":"\n\n> Now, the Test Statistic is less than the Critical Value and the p-value is less than 5%. So, we can be confident that the trend is almost removed.\n\n"},{"metadata":{"id":"TyaHZgxpBf5y"},"cell_type":"markdown","source":"![alt text](https://)\n\n> ***Now  we should stabilize the variance, b.c. it is also a requirement of stationary time series***\n\n\n"},{"metadata":{"id":"-DvOvn9LA1uE","outputId":"6ccd7136-bca2-489c-8024-fd0b98b255f2","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,14))\n\ntrain_log_diff = train_log - train_log.shift(1) \ntest_stationarity(train_log_diff.dropna())","execution_count":null,"outputs":[]},{"metadata":{"id":"DRj1OghYCQ_4"},"cell_type":"markdown","source":"\n\n> Now we will decompose the time series into trend and seasonality and will get the residual which is the random variation in the series.\n\nWe are removing the seasonal part \n\n"},{"metadata":{"id":"mawOsCg9CbIB","outputId":"32a20424-0ac2-465a-c2ef-6cfb5a0c7cfd","trusted":false},"cell_type":"code","source":"from pmdarima import auto_arima\nmodel = auto_arima(train_log, trace=True, error_action='ignore', suppress_warnings=True)\nmodel.fit(train_log)\nforecast = model.predict(n_periods=len(test))\nforecast = pd.DataFrame(forecast,index = test_log.index,columns=['Prediction'])\n#plot the predictions for validation set\nplt.plot(train_log, label='Train')\nplt.plot(test_log, label='Test')\nplt.plot(forecast,'g--', label='Prediction')\nplt.title('Wallmart Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Actual Stock Price')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"2x1Cr8ReLJcn"},"cell_type":"markdown","source":"\n\n> We will use the RMSE(Root Mean Square Error) to judge our forecast results\n\n"},{"metadata":{"id":"zojUiBe7LRHp","outputId":"04956b93-0311-4561-c35a-bf71e744b563","trusted":false},"cell_type":"code","source":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nrms = sqrt(mean_squared_error(test_log,forecast))\nprint(\"RMSE: \", rms)","execution_count":null,"outputs":[]},{"metadata":{"id":"zGVK5wxb2_Zm","outputId":"3f302de6-d416-46a8-d6fd-ce5093c9c4d9","trusted":false},"cell_type":"code","source":"# Calculate the absolute errors\nerrors = abs(np.ravel(forecast) - np.ravel(test_log))\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"id":"sUcuvlBl3IN5","outputId":"d24a77dc-c2c4-4604-b01c-816ced3eb61b","trusted":false},"cell_type":"code","source":"# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (np.ravel(errors) / np.ravel(test_log))\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Prediction Accuracy is:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"AElw7Z1LLYHb"},"cell_type":"markdown","source":"**Well, 1.1% of error is a VERY GOOD result in our case**"},{"metadata":{"id":"8iPD0rSWpFIW","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats.mstats import gmean","execution_count":null,"outputs":[]},{"metadata":{"id":"UUaWovLeptaa","trusted":false},"cell_type":"code","source":"price_df = pd.read_csv('/content/drive/My Drive/prices.csv')\nsec_df = pd.read_csv('/content/drive/My Drive/securities.csv')\nfund_df = pd.read_csv('/content/drive/My Drive/fundamentals.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"usSsINryp6ZG","outputId":"6386a88f-f1d8-4133-fe86-63b92c927f3a","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nax = sns.countplot(y='GICS Sector', data=sec_df)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"id":"ExDrJZJ3qBma","outputId":"aff66c05-609f-49f1-d69e-454884220de0","trusted":false},"cell_type":"code","source":"sec_df = sec_df.rename(columns = {'Ticker symbol' : 'symbol','GICS Sector' : 'sector'})\nsec_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"zQAOAEhAqOih","outputId":"3b799e5d-85d6-44e8-c3d8-eb6019deed4c","trusted":false},"cell_type":"code","source":"price_df  = price_df.merge(sec_df[['symbol','sector']], on = 'symbol')\nprice_df['date'] = pd.to_datetime(price_df['date'])\nprice_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"dEQx8tV6qVU2","trusted":false},"cell_type":"code","source":"price_df = price_df[price_df['date'] <= '2015-12-15']","execution_count":null,"outputs":[]},{"metadata":{"id":"p-EDosZeqbeA","outputId":"472d5b2c-8cb7-4831-f6d1-3357df5f7407","trusted":false},"cell_type":"code","source":"sector_pivot = pd.pivot_table(price_df, values = 'open', index = ['date'],columns = ['sector']).reset_index()\nsector_pivot","execution_count":null,"outputs":[]},{"metadata":{"id":"4AGKTuxPqxNh","outputId":"c0124943-f235-4c92-fec6-5a4833c173b2","trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.heatmap(sector_pivot.corr('kendall'),annot=True, cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zfvgVFbpq8Ro","outputId":"8041f37c-8230-4af6-8f09-c4ba65cc168c","trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.heatmap(sector_pivot.corr('spearman'),annot=True, cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"MbgRIsv8rCv0","outputId":"bbaff3e1-3a34-4b70-e4ec-d349e0308992","trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.heatmap(sector_pivot.corr('pearson'),annot=True, cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"tf_l74Fdt02A","trusted":false},"cell_type":"code","source":"#importing lib\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly.graph_objs import *\nimport plotly.graph_objs as go\n# init_notebook_mode()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"id":"IZLgOsX5rbzc","trusted":false},"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"id":"bw3FslAIrem6","outputId":"131cbbe7-c3b2-4157-aaf0-ecad997bf91f","trusted":false},"cell_type":"code","source":"x, y = ecdf(price_df['open'])\n\nsamples = np.random.normal(np.mean(price_df['open']), np.std(price_df['open']), size=10000)\nx_theor, y_theor = ecdf(samples)\nsns.set()\nplt.figure(figsize=(12,14))\nplt.plot(x_theor, y_theor)\n\nplt.plot(x, y, marker=\".\", linestyle=\"none\")\n\nplt.legend(('Normal Distribution', 'Opening costs empirical Data'), loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"id":"50lfg-m5sEUA","outputId":"6930b10f-0ea9-4e49-ad9f-3fa528f5efb1","trusted":false},"cell_type":"code","source":"x, y = ecdf(price_df['close'])\n\nsamples = np.random.normal(np.mean(price_df['close']), np.std(price_df['close']), size=10000)\nx_theor, y_theor = ecdf(samples)\nsns.set()\nplt.figure(figsize=(12,14))\nplt.plot(x_theor, y_theor)\n\nplt.plot(x, y,'go')\n\nplt.legend(('Normal Distribution', 'Closing costs empirical Data'), loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"id":"lV4eCSxFs_Cy","outputId":"06131d43-db0b-4b73-c647-b04784994401","trusted":false},"cell_type":"code","source":"x, y = ecdf(price_df['volume'])\n\nsamples = np.random.normal(np.mean(price_df['volume']), np.std(price_df['volume']), size=10000)\nx_theor, y_theor = ecdf(samples)\nsns.set()\nplt.figure(figsize=(12,14))\nplt.plot(x_theor, y_theor)\n\nplt.plot(x, y,'r--')\n\nplt.legend(('Normal Distribution', 'Costs volume empirical Data'), loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"id":"6J84jy1StNph","outputId":"6f4a5114-0111-42c9-deca-ba24960af1c2","trusted":false},"cell_type":"code","source":"x, y = ecdf(price_df['high'])\n\nsamples = np.random.normal(np.mean(price_df['high']), np.std(price_df['high']), size=10000)\nx_theor, y_theor = ecdf(samples)\nsns.set()\nplt.figure(figsize=(12,14))\nplt.plot(x_theor, y_theor)\n\nplt.plot(x, y,'y.')\n\nplt.legend(('Normal Distribution', 'Highest costs empirical Data'), loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"id":"mKAS-X7ytc0a","outputId":"3fab82b0-7e58-4758-8b40-c605426c525c","trusted":false},"cell_type":"code","source":"x, y = ecdf(price_df['low'])\n\nsamples = np.random.normal(np.mean(price_df['low']), np.std(price_df['low']), size=10000)\nx_theor, y_theor = ecdf(samples)\nsns.set()\nplt.figure(figsize=(12,14))\nplt.plot(x_theor, y_theor,'r-')\n\nplt.plot(x, y,'b.')\n\nplt.legend(('Normal Distribution', 'Least costs empirical Data'), loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"id":"r0e9YgeQ0m36","trusted":false},"cell_type":"code","source":"from __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"id":"Di0Gu4QFtn48"},"cell_type":"markdown","source":"Hence, we can see that all costs data features are distributed closely to the Normal Gaussian rule\n"},{"metadata":{"id":"IrUw5rQ_un4Q","trusted":false},"cell_type":"code","source":"stocks = fund_df","execution_count":null,"outputs":[]},{"metadata":{"id":"rKFb5wtmu7GJ","trusted":false},"cell_type":"code","source":"top_rev = stocks.groupby(by='Ticker Symbol').agg({'Total Revenue':sum})","execution_count":null,"outputs":[]},{"metadata":{"id":"qGvqzlHavExQ","trusted":false},"cell_type":"code","source":"g = top_rev['Total Revenue'].nlargest(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"M7AVJtOpvJLc","outputId":"a3741da7-9dc2-48e5-b13d-e653ce7ebdcc","trusted":false},"cell_type":"code","source":"next = [Bar(\n            y=g,\n            x=g.keys(),\n            marker = dict(\n            color = 'lightsteelblue'\n            ),\n            name = \"Contractor's amount earned per project\"\n    )]\nlayout1 = go.Layout(\n    title=\"Top 10 Exporters\",\n    xaxis=dict(\n        title='Company',\n        titlefont=dict(\n            size=30,\n            color='#7f7f7f'\n               )\n    ),\n    yaxis=dict(\n        title='Total Revenue',\n        titlefont=dict(\n            size=22,\n            color='#7f7f7f'\n        )\n    )\n)\nmyFigure2 = go.Figure(data = next, layout = layout1)\niplot(myFigure2)","execution_count":null,"outputs":[]},{"metadata":{"id":"0u-0D9Olvfv2"},"cell_type":"markdown","source":"Here are the top 5 stocks that has the biggest revenue: \n\n    1. Walmart \n    2. EXXON MOBIL corp\n    3. Apple inc.\n    4. Chevron Corp.\n    5. General Motoros"},{"metadata":{"id":"guJzqtVMva_C","trusted":false},"cell_type":"code","source":"WMT = stocks[stocks['Ticker Symbol']=='WMT']\n#sns.distplot(Aqua['Generation'],bins=28,kde=False,color='red')","execution_count":null,"outputs":[]},{"metadata":{"id":"n6vE4SbwvpHh","trusted":false},"cell_type":"code","source":"gir = ['Total Equity',\n       'Total Revenue',\n       'Accounts Payable',\n       'Accounts Receivable',\n      'Cost of Revenue',\n      'Profit Margin',\n      'Sale and Purchase of Stock',\n      'Earnings Per Share',\n       'Net Borrowings']\ntip = np.corrcoef(WMT[gir].values.T)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZtId4hoBvtxX","outputId":"e9f3e97e-1a25-4944-8583-5d9f02aa4453","trusted":false},"cell_type":"code","source":"WMT.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"OAKJBafgvyS4","outputId":"fe9bcdec-7ad4-434c-d781-7437840dcaae","trusted":false},"cell_type":"code","source":"sns.set(font_scale = 0.9)\nplt.figure(figsize=(12,14))\n\nmap = sns.heatmap(tip, cbar = True,\n                  cmap=\"YlGnBu\",\n                  annot = True, \n                  square= True,\n                  fmt = '.1f',\n                  annot_kws = {'size':11}, \n                 yticklabels = gir,\n                 xticklabels = gir)","execution_count":null,"outputs":[]},{"metadata":{"id":"FVuEXipPwED9"},"cell_type":"markdown","source":"#  **Linear Regression**"},{"metadata":{"id":"IsWigXUwv5DE","trusted":false},"cell_type":"code","source":"\nn = WMT['Total Revenue']\nm = WMT[['Total Equity',\n       'Accounts Payable',\n       'Accounts Receivable',\n      'Cost of Revenue',\n      'Profit Margin',\n      'Sale and Purchase of Stock',\n      'Earnings Per Share',\n       'Net Borrowings']]\ntarget=['Total Revenue']\nfeatures = ['Total Equity',\n       'Accounts Payable',\n       'Accounts Receivable',\n      'Cost of Revenue',\n      'Profit Margin',\n      'Sale and Purchase of Stock',\n      'Earnings Per Share',\n       'Net Borrowings']","execution_count":null,"outputs":[]},{"metadata":{"id":"nzABXMq-w2Li","trusted":false},"cell_type":"code","source":"# Splitting the into sets of training and test.\ntrain,test,train_label,test_label=train_test_split(m,n,test_size=0.33,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"id":"Urll5lKRxOrT","outputId":"e13eacf0-c399-4674-a4d4-a328ac09f7c8","trusted":false},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(train, train_label)\n# coef_ - is an array of features\nprint(np.ravel(lr.coef_))","execution_count":null,"outputs":[]},{"metadata":{"id":"J0J1TRa9zRqy","outputId":"173bb242-af44-49b7-cecf-a237309a1dfa","trusted":false},"cell_type":"code","source":"Linear = LinearRegression(fit_intercept=True)\nmo = Linear.fit(train,train_label)\npredi = mo.predict(test)\nprint(r2_score(test_label,predi))","execution_count":null,"outputs":[]},{"metadata":{"id":"rNI2szRLyWEw","outputId":"afca0617-aa2d-491a-e976-20315beca653","trusted":false},"cell_type":"code","source":"print('Correlation: ', round(np.corrcoef(np.ravel(test_label), np.ravel(predi))[0,1], 5))\nfig = plt.figure(figsize=(10, 6))\nplt.title('Correlation between predicted and actual results (Linear Regressor)')\nplt.plot(test_label, predi, 'r*')\nplt.xlabel('actual')\nplt.ylabel('predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"t7tQq6ftzc6N","outputId":"3099d314-75f2-4781-b27d-318fc3530967","trusted":false},"cell_type":"code","source":"# Finding the coefficient. (value of 1 unit increase) \ncoef = pd.DataFrame(Linear.coef_,m.columns,columns=['Coefficient'])\ncoef","execution_count":null,"outputs":[]},{"metadata":{"id":"XwzpkPdTzjN9","outputId":"28d845c8-dc62-48db-b016-9cd23b6ff77c","trusted":false},"cell_type":"code","source":"errors = abs(np.ravel(predi) - np.ravel(test_label))\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"id":"xSpCX-Bp14fG","outputId":"d78806e2-ab3e-4a11-ef87-15adf99c382a","trusted":false},"cell_type":"code","source":"# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (np.ravel(errors) / np.ravel(test_label))\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"-mNUpFj_XQFd"},"cell_type":"markdown","source":"# **Polynomial regression**"},{"metadata":{"id":"AofjTJPA2ejh","trusted":false},"cell_type":"code","source":"# Polynomial regression\n# Fit on train set\nmodel = PolynomialFeatures(degree=2)\nX_train_ = model.fit_transform(test)\nX_test_ = model.fit_transform(test)\n\nplr = LinearRegression()\nplr.fit(X_train_, train_label)\npredicted_data = plr.predict(X_test_)\n\n\n# predicted_data = np.round_(predicted_data)\n# correct result; round of prediction, prediction\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5Pv6gmZu3Q8D","outputId":"8eb3da6e-1a43-49a4-97f1-d0868d04b2b9","trusted":false},"cell_type":"code","source":"print('R^2: ', plr.score(X_test_, test_label))","execution_count":null,"outputs":[]},{"metadata":{"id":"s9NwIuvO3fXN","outputId":"398efb96-3528-43d0-b5af-f5879ce5c4f7","trusted":false},"cell_type":"code","source":"RMSE = sqrt(mean_squared_error(y_true=test_label, y_pred=predicted_data))\nprint('RMSE: ', RMSE)","execution_count":null,"outputs":[]},{"metadata":{"id":"E21kH0IW3xdl","outputId":"01511db1-7055-4811-ae7c-03d5404b452d","trusted":false},"cell_type":"code","source":"print('squared errors: ', \n      round(sum(np.ravel(abs(test_label - np.around(predicted_data)))) / len(test_label), 3))","execution_count":null,"outputs":[]},{"metadata":{"id":"jtxI4EIU4jDv","outputId":"3726a934-0e07-48ce-cb9a-9a8501343cf5","trusted":false},"cell_type":"code","source":"errors = abs(np.ravel(predicted_data) - np.ravel(test_label))\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"id":"M4yIqod238Up","outputId":"1de838fa-779f-43b5-8d47-d715cf7e634c","trusted":false},"cell_type":"code","source":"mape = 100 * (np.ravel(errors) / np.ravel(test_label))\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Polynomial Regression Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"N62DgE7pXeWz"},"cell_type":"markdown","source":"# **Random Forest**"},{"metadata":{"id":"QLU2PDET4ssA","trusted":false},"cell_type":"code","source":"# Train Model\n# Instantiate model \nrfg = RandomForestRegressor(n_estimators= 1000, random_state=42, criterion = 'mse', max_depth = None,\n                            min_samples_split = 2, min_samples_leaf = 1)\n\n# Train the model on training data\nrfg.fit(train, np.ravel(train_label));\n\n# Make Predictions on Test Data\n# Use the forest's predict method on the test data\ny_prediction = rfg.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KT46ZXwz5OGz","outputId":"08ee50e9-f019-4bdd-db43-6329e1a037a8","trusted":false},"cell_type":"code","source":"print('Correlation: ', round(np.corrcoef(np.ravel(test_label), np.ravel(y_prediction))[0,1], 5))\nfig = plt.figure(figsize=(10, 6))\nplt.title('Correlation between actual and predicted results (Random Forest Regressor)')\nplt.plot(test_label, y_prediction, 'r*')\nplt.xlabel('actual')\nplt.ylabel('predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KDxYGSj65cPm","outputId":"becbcd2f-7328-45f8-fbaf-8b50b5a98181","trusted":false},"cell_type":"code","source":"print('R^2: ', rfg.score(test, test_label))","execution_count":null,"outputs":[]},{"metadata":{"id":"w8Iqixfm5wk9","outputId":"0b667ebd-789b-4c0a-91a7-6c734dfef5b8","trusted":false},"cell_type":"code","source":"RMSE = sqrt(mean_squared_error(y_true=test_label, y_pred=y_prediction))\nprint('RMSE: ', RMSE)","execution_count":null,"outputs":[]},{"metadata":{"id":"BWRrLFCH50M1","outputId":"d07905e1-ce54-4611-f319-947bb93b1b4a","trusted":false},"cell_type":"code","source":"print('squared errors: ', \n      round(sum(np.ravel(abs(np.ravel(test_label) - np.around(y_prediction)))) / len(test_label), 3))","execution_count":null,"outputs":[]},{"metadata":{"id":"_or8_R8f57_h","outputId":"b0e9a8ef-34b4-4b28-e4c3-128f13afcc65","trusted":false},"cell_type":"code","source":"errors = abs(np.ravel(y_prediction) - np.ravel(test_label))\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (np.ravel(errors) / np.ravel(test_label))\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"5iC6sHKy6aYZ","outputId":"0204f68c-a105-4f9f-fa8b-eb75d94810e4","trusted":false},"cell_type":"code","source":"tree = rfg.estimators_[50]\nprint('The depth of this tree is:', tree.tree_.max_depth)","execution_count":null,"outputs":[]},{"metadata":{"id":"mK-OYaxP6cem","outputId":"2004c0d2-5d97-494a-a2fa-2ec0506fabdc","trusted":false},"cell_type":"code","source":"# Variable Importances\n# Get numerical feature importances\nimportances = list(rfg.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(m, round(importance, 2)) for m, importance in zip(m, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","execution_count":null,"outputs":[]},{"metadata":{"id":"o0dU4XaM7KPA","outputId":"f8260152-fc1d-4921-9500-a07512acbdaa","trusted":false},"cell_type":"code","source":"x_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, features, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance'); plt.title('Feature Importances');","execution_count":null,"outputs":[]},{"metadata":{"id":"Dec1zaDT7VrC","outputId":"7298f8dc-560d-4f20-ce20-d38abcbc04e1","trusted":false},"cell_type":"code","source":"# A cumulative importance graph: shows the contribution to the overall importance \n# of each additional variable. \n# The dashed line - at 95% of total importance accounted for.\n# After that some unimportant features (sulphates and may be density) can be removed.\n# 95% - is an arbitrary threshold.\nplt.figure(figsize=(12,14))\n# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","execution_count":null,"outputs":[]},{"metadata":{"id":"ZFRFloxV7yk9","outputId":"60a7cf48-f5d2-402f-b630-1cc1c7ed6a04","trusted":false},"cell_type":"code","source":"# Find number of features for cumulative importance of 95%\n# Add 1 because Python is zero-indexed\nimportantFeaturesCount = np.where(cumulative_importances > 0.95)[0][0] + 1\nprint('Count of features for 95% importance:', importantFeaturesCount)\n\n# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:importantFeaturesCount]]\n# Find the columns of the most important features\nimportant_indices = [features.index(feature) for feature in important_feature_names]\n# Create training and testing sets with only the important features\nimportant_train_features = train.iloc[:, important_indices]\nimportant_test_features = test.iloc[:, important_indices]\n# Sanity check on operations\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_feature_names)","execution_count":null,"outputs":[]},{"metadata":{"id":"8qA9adcAEM9d","outputId":"0bed349b-21e2-49cd-d177-79922f1c9ba8","trusted":false},"cell_type":"code","source":"important_feature_names","execution_count":null,"outputs":[]},{"metadata":{"id":"Oe2q66XbA8DL","outputId":"9e3de90b-2dc7-41e8-fde3-7f40fa2bc37f","trusted":false},"cell_type":"code","source":"# As it can be seen from the results, they became worse than for the case with all the features\n# Removing the so-called \"unimportant\" feature did not improve metrics\n\n# Training and Evaluating on Important Features\n# Train the expanded model on only the important features\nrfg.fit(important_train_features, np.ravel(train_label));\n# Make predictions on test data\npredictions = rfg.predict(important_test_features)\n# Performance metrics\nprint('R^2: ', rfg.score(important_test_features, test_label))\nerrors = abs(np.ravel(predictions) - np.ravel(test_label))\nprint('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (np.ravel(errors) / np.ravel(test_label))\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"1rg-J342X20Z"},"cell_type":"markdown","source":"# **Testing Classification prediction on different training samples**"},{"metadata":{"id":"dHfb-2-50Ogj","outputId":"c735f3b9-d1c4-45d3-c004-4341f4dccaaf","trusted":false},"cell_type":"code","source":"reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=10)\nreg3 = LinearRegression()\nereg = VotingRegressor([('gb', reg1), ('rf', reg2), ('lr', reg3)])\nreg1.fit(train, train_label.values.ravel())\nreg2.fit(train, train_label.values.ravel())\nreg3.fit(train, train_label.values.ravel())\nereg.fit(train, train_label.values.ravel())\n\n# for some 20 values\nxt = train[:20]\n\nplt.figure(figsize=(10, 6))\nplt.plot(reg1.predict(xt), 'gd', label='GradientBoostingRegressor')\nplt.plot(reg2.predict(xt), 'b^', label='RandomForestRegressor')\nplt.plot(reg3.predict(xt), 'ys', label='LinearRegression')\nplt.plot(ereg.predict(xt), 'r*', label='VotingRegressor')\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Comparison of individual predictions with averaged')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KuRsuZQr00Bz","outputId":"1c6626d6-b267-4d2b-a6c2-7df2ef32dd9c","trusted":false},"cell_type":"code","source":"xt = test[:20]\n\nplt.figure(figsize=(10, 6))\nplt.plot(reg1.predict(xt), 'gd', label='GradientBoostingRegressor')\nplt.plot(reg2.predict(xt), 'b^', label='RandomForestRegressor')\nplt.plot(reg3.predict(xt), 'ys', label='LinearRegression')\nplt.plot(ereg.predict(xt), 'r*', label='VotingRegressor')\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Comparison of individual predictions with averaged')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MMtVXKiCkfwX","outputId":"0dedd56a-cf3c-45f3-e646-cb8fd425d741","trusted":false},"cell_type":"code","source":"# KNN\nmainFrame = pd.read_csv('/content/drive/My Drive/prices.csv')\n\nmainFrame[\"change\"] = mainFrame[\"close\"] - mainFrame[\"open\"]\n\ndf_interest = mainFrame[[\"symbol\", \"date\", \"open\", \"close\", \"change\", \"volume\"]]\n\ndf_interest[\"date\"] = pd.to_datetime(df_interest[\"date\"])\n\ndf_interest.head()\n\nsymbols = df_interest[\"symbol\"].unique().tolist()\n\nfull_count = len(symbols)\n\n\n\noppFrame = df_interest.pivot(index = 'date', columns = 'symbol', values = 'close')\n\noppFrame = oppFrame.dropna(axis=1)\n\nopp_symbols = oppFrame.columns\n\npart_count = len(oppFrame.columns) \n\n'''\n\nfor u in opp_symbols[:10]:\n\n    plt.plot(oppFrame.index.tolist(), oppFrame[u].values.tolist())\n\nplt.legend(symbols, loc='upper left')\n\nplt.show()\n\n'''\n\n\n\n\nss = df_interest.groupby(by=[\"symbol\"])[\"change\"].std()\n\nss = (ss-ss.mean())/ss.std()\n\n\n\npcs = df_interest.groupby(by='symbol').apply(lambda grp: grp[grp['change'] > 0]['change'].count() / grp['change'].size)\n\npcs = (pcs-pcs.mean())/pcs.std()\n\n\n\navgv = df_interest.groupby(by=['symbol'])['volume'].mean()/10000000\n\navgv = (avgv-avgv.mean())/avgv.std()\n\n\n\nnewdf = pd.concat([ss, pcs, avgv], axis=1).reset_index()\n\nnewdf.columns = ['symbol', 'std', 'prop_pos_day_change', \"avg_volume\"]\n\nnewdf.head()\n\n\n\nfor i in newdf['symbol'].tolist():\n\n    x = newdf[newdf['symbol'] == i]['std']\n\n    y = newdf[newdf['symbol'] == i]['prop_pos_day_change']\n\n    plt.scatter(x,y)\n\nplt.legend(newdf['symbol'].tolist(),\n\n           bbox_to_anchor=(1.05, 1),\n\n           loc=2,\n\n           borderaxespad=0.,\n\n          ncol=10)\n\n\n\nplt.title(r'Stock Change and Spread', fontsize=32)\n\nplt.xlabel('Daily Positive Change (%)', fontsize=22)\n\nplt.ylabel('Total Standard Devation', fontsize=22)\n\nplt.figure(figsize=(10,100))\n\nplt.show()\n\n\n\ndf1 = newdf.iloc[0:10,:]\n\ndf2 = newdf.iloc[100:110,:]\n\ndf3 = newdf.iloc[200:210,:]\n\ndf4 = newdf.iloc[300:310,:]\n\ndf5 = newdf.iloc[400:410,:]\n\ntestdf = pd.concat([df1,df2,df3,df4,df5])\n\nfor i in range(0,50):\n\n    testdf.index.values[i] = i\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmdf = testdf\n\nmet={}\n\n# Visualize K = {3..9}\n\nkValues = [i for i in range(3,10)]\n\nfor k in kValues:\n\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(kmdf[['std','prop_pos_day_change']].to_numpy())\n\n    kmdf[str(k)] = kmeans.labels_\n\n\n\nkmdf = pd.melt(kmdf, \n\n                id_vars=[\"symbol\", 'std', 'prop_pos_day_change'],\n\n                var_name=\"k\", \n\n                value_name=\"values\",\n\n                value_vars=list(kmdf.columns[-7:]))\n\n\n\nkmdf.head()\n\n\n\ng = sns.FacetGrid(kmdf, col=\"k\", hue=\"values\", col_wrap=4, palette='Set2')\n\ng = g.map(plt.scatter, \"std\", \"prop_pos_day_change\")\n\ng.set(xlabel=\"Closing Deviation\")\n\ng.set(ylabel=\"'Daily Positive Change (%)\")\n\ng.fig.suptitle(\"Stock Cluster Analysis\", size=28)\n\ng.fig.subplots_adjust(top=.8)\n\nplt.subplots_adjust(hspace=1.2, wspace=0.4)\n\ng.add_legend()\n\ng._legend.set_title(\"Cluster\")\n\n#handles = g._legend_data.values()\n\n#labels = g._legend_data.keys()\n\n#g.fig.legend(handles=handles, labels=labels, loc='lower right', ncol=3)\n\nmet={}\nfrom sklearn import metrics\n\nfor i in range(4,10):\n\n    met[str(i)] = metrics.silhouette_score(kmdf.loc[kmdf['k']==str(i)][['std','prop_pos_day_change']], kmdf.loc[kmdf['k']==str(i)]['values'], metric='euclidean')\n\n\n\nmetdf = pd.Series(met)","execution_count":null,"outputs":[]},{"metadata":{"id":"NUSEBysY5iti"},"cell_type":"markdown","source":"# **Factor Analysis with PCA**"},{"metadata":{"id":"iWgtXixS4hl-","outputId":"7765fe49-4c77-46e9-a335-d6e5fb3832af","trusted":false},"cell_type":"code","source":"stocks.isnull().any()\nnewdata = pd.DataFrame(stocks[gir]) \nnewdata","execution_count":null,"outputs":[]},{"metadata":{"id":"1XPaJIiQ5g6f","outputId":"fb515a73-0008-4bbe-a1b8-6ef7df049a78","trusted":false},"cell_type":"code","source":"X = newdata.interpolate()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"v4PLQzpf8PsV","outputId":"c8e2dcb4-a53b-46fe-d8aa-d28fbe6b0103","trusted":false},"cell_type":"code","source":"X = X.dropna()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"IOvrVmrc8VOw","outputId":"43aaae01-4d17-424e-8c7c-800137075faa","trusted":false},"cell_type":"code","source":"def plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\nx_before_pca = pd.DataFrame(X)\nx_before_pca.describe()\nx_before_pca.shape\nplot_correlation_map(x_before_pca)\nfrom sklearn.decomposition import PCA as PCA\npca = PCA(n_components=9)\npca.fit(x_before_pca)\nvar = pca.explained_variance_ratio_\nvar1 = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nvar1\nplt.plot(var1)\nx_pca = PCA(n_components=3)\nx_pca.fit(x_before_pca)\nx = x_pca.fit_transform(x_before_pca)\nd = {'pc1': x[:,0], 'pc2': x[:, 1], 'pc3': x[:,2]}\nx_df = pd.DataFrame(d)\nx_df.head(3)\nx_df.describe()\nx_new_ndarray = x_pca.inverse_transform(x_df)\nx_new = pd.DataFrame(x_new_ndarray)\nx_new.columns = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']\nx_new.head(3)\nx_before_pca.head(3)\nplt.scatter(x_df['pc1'], x_df['pc2'], color = 'green')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8zVRKwQW-zWq","outputId":"b53bed34-8316-4577-f97d-c5d2672c9d47","trusted":false},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(xs = x_df['pc1'], ys = x_df['pc2'], zs= x_df['pc3'], zdir='z')","execution_count":null,"outputs":[]},{"metadata":{"id":"QFoMY5wW_CSC","outputId":"b42c3c8d-0f2c-4446-eb91-269d340279e3","trusted":false},"cell_type":"code","source":"x_df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4d9zAdL1_GOI","outputId":"a884490b-5f98-4d04-9c28-7d413e346932","trusted":false},"cell_type":"code","source":"x_df","execution_count":null,"outputs":[]},{"metadata":{"id":"O8BgJ012-6sH","outputId":"a992d8fe-b097-4167-c02b-87d19b7cfff4","trusted":false},"cell_type":"code","source":"plt.scatter(x_df['pc1'], x_df['pc2'], color = 'red')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"n2jH4q4j_91D"},"cell_type":"markdown","source":"# **ANOVA**(**AN**alysis **O**f **VA**riance)"},{"metadata":{"id":"VySsJarIAU4E","outputId":"4a26bb2f-57fb-4fd3-bd93-5275ee30260c","trusted":false},"cell_type":"code","source":"# load packages\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# load data file\nd = stocks\n# d['Date Local']=pd.to_datetime(d['Date Local'])\n# reshape the d dataframe suitable for statsmodels package \n# you do not need to reshape if your data is already in stacked format. Compare d and d_melt tables for detail \n# understanding \nd_melt = pd.melt(d, id_vars=['Total Revenue'], value_vars=['Total Equity',\n       'Accounts Payable',\n       'Accounts Receivable',\n      'Cost of Revenue',\n      'Profit Margin',\n      'Sale and Purchase of Stock',\n      'Earnings Per Share',\n       'Long-Term Investments',\n       'Net Borrowings'])\n# replace column names\n\nd_melt.columns = ['Revenue', 'features', 'value']\n# generate a boxplot to see the data distribution by genotypes and years. Using boxplot, we can easily detect the \n# differences between different groups\nsns.set(rc={'figure.figsize':(20.7,22.27)})\nsns.scatterplot(x=\"Revenue\", y=\"value\", hue=\"features\", data=d_melt)","execution_count":null,"outputs":[]},{"metadata":{"id":"gEXe9wdGOT29","trusted":false},"cell_type":"code","source":"import scipy.stats as stats\nfrom statsmodels.formula.api import ols\nresults = ols('value ~ C(Revenue) + C(features)',data=d_melt).fit()","execution_count":null,"outputs":[]},{"metadata":{"id":"niWyW2QwYqqf","outputId":"d1dc2fc4-3e96-49a9-eb4e-86a035321971","trusted":false},"cell_type":"code","source":"results.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"5Kv_lvQvY4En","outputId":"959dc870-1f78-4c78-ea92-18ee9da643a9","trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\n\nanova_table = sm.stats.anova_lm(results, typ=2)\nanova_table","execution_count":null,"outputs":[]},{"metadata":{"id":"ZY5GNN0Hay0X","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"id":"I9yyghsjfv5z"},"cell_type":"markdown","source":"# **Classification**\n"},{"metadata":{"id":"bGTATq39cFmT","trusted":false},"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(train)\nX_test = sc.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Blg4w1Xrcf1r","outputId":"096ffdcd-6cc3-4e6a-dc41-196e88ea1f05","trusted":false},"cell_type":"code","source":"\n###\n# Random Forest Classifier\n# A random forest is a meta estimator that fits a number of decision tree classifiers on various\n# sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n# The sub-sample size is always the same as the original input sample size\n# but the samples are drawn with replacement if bootstrap=True (default).\n# n_estimators - The number of trees on the forest\n\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(train, np.ravel(train_label))\npred_rfc = rfc.predict(test)\n# print some values (correct and predicted ones)\nprint('correct:  ', np.ravel(test_label.values[0:15]))\nprint('predicted:', pred_rfc[0:15])","execution_count":null,"outputs":[]},{"metadata":{"id":"RHTob5wJc4jt","outputId":"25bbba5d-b1e6-4d73-b837-fc816d02a6e7","trusted":false},"cell_type":"code","source":"for i in range(0,11):\n    print('     ' + str(i), end='')\nprint('\\n')\npred_prob = rfc.predict_proba(X_test)[0:15]\n# print correct result and predicted probabilities for each value\n","execution_count":null,"outputs":[]},{"metadata":{"id":"yxtaCnlvdlCq","outputId":"090c2f30-92d2-4c7a-a69b-dd753920560f","trusted":false},"cell_type":"code","source":"print('Correlation: ', round(np.corrcoef(np.ravel(test_label), np.ravel(pred_rfc))[0,1], 5))\nfig = plt.figure(figsize=(10, 6))\nplt.title('Correlation between predicted and actual results (Random Forest Classifier)')\nplt.plot(test_label, pred_rfc, 'r*')\nplt.xlabel('actual')\nplt.ylabel('predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3k9BW_8AgAbE","outputId":"23d84f4a-4a6f-4c18-b0a5-3e3359ea0e24","trusted":false},"cell_type":"code","source":"# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\n\n\nError = np.mean(np.abs(np.subtract(test_label,pred_rfc)))\nAverage = np.mean(test_label)\nMAPE = (Error/Average)*100\n\nprint('MAPE=',MAPE,'%.')\nprint('Accuracy=',100-MAPE,'%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"efcWDQgpkHIV","outputId":"d2c755f1-32f0-47b8-b391-648c294c6145","trusted":false},"cell_type":"code","source":"###\n# Stochastic Gradient Descent Classifier\n# The advantages of SGD:\n# - Efficiency.\n# - Ease of implementation (lots of opportunities for code tuning).\n# The disadvantages of SGD:\n# - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n# - SGD is sensitive to feature scaling.\n\nsgd = SGDClassifier(penalty=\"elasticnet\", max_iter=2000, tol=0.00001, loss=\"modified_huber\")\nsgd.fit(X_train, np.ravel(train_label))\npred_sgd = sgd.predict(test)\n# print(sgd.coef_)\n# print some values (correct and predicted ones)\nprint('correct:  ', np.ravel(test_label.values[0:15]))\nprint('predicted:', np.ravel(pred_sgd[0:15]))","execution_count":null,"outputs":[]},{"metadata":{"id":"z1wlFj5hkT3T","outputId":"f9241395-f1b7-49c3-82b7-493beb982730","trusted":false},"cell_type":"code","source":"print('Correlation: ', round(np.corrcoef(np.ravel(test_label), np.ravel(pred_sgd))[0,1], 5))\nfig = plt.figure(figsize=(10, 6))\nplt.title('Correlation between predicted and actual results (Stochastic Gradient Descent Classifier)')\nplt.plot(test_label, pred_sgd, 'r*')\nplt.xlabel('actual')\nplt.ylabel('predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"x0KHzl26kiti","outputId":"e4448e51-838d-4b36-c973-dce0fea3a75e","trusted":false},"cell_type":"code","source":"\nprint(classification_report(test_label, pred_sgd))\n\nprint(confusion_matrix(test_label, pred_sgd))","execution_count":null,"outputs":[]},{"metadata":{"id":"jgyXPnkZkwZ9","outputId":"32d44d7d-a8da-4f62-8943-49dca5d5019a","trusted":false},"cell_type":"code","source":"# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\n\n\nError = np.mean(np.abs(np.subtract(test_label,pred_sgd)))\nAverage = np.mean(test_label)\nMAPE = (Error/Average)*100\n\nprint('MAPE=',MAPE,'%.')\nprint('Accuracy=',100-MAPE,'%.')","execution_count":null,"outputs":[]},{"metadata":{"id":"Vdjyfym9k6Lp","outputId":"f30e7e97-4b5e-45b9-947b-bd39f5206de2","trusted":false},"cell_type":"code","source":"###\n# Support Vector Classifier\n# fit time complexity is more than quadratic with the number of samples -> hard to scale more than 10000 samples.\n# probability=True - to use predict_proba method\n\nsvc = SVC(probability=True)\nsvc.fit(train, np.ravel(train_label))\npred_svc = svc.predict(test)\n# print some values (correct and predicted ones)\nprint('correct:  ', np.ravel(test_label.values[0:15]))\nprint('predicted:', pred_svc[0:15])","execution_count":null,"outputs":[]},{"metadata":{"id":"St0Oe-2SlaO9","outputId":"0f14a25d-e5ff-48fb-be6e-3f71334f8943","trusted":false},"cell_type":"code","source":"print('Correlation: ', round(np.corrcoef(np.ravel(test_label), np.ravel(pred_svc))[0,1], 5))\nfig = plt.figure(figsize=(10, 6))\nplt.title('Correlation between predicted and actual results (Support Vector Classifier)')\nplt.plot(test_label, pred_svc, 'r*')\nplt.xlabel('actual')\nplt.ylabel('predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5qNgjy7zlpyV","outputId":"79faa524-94c5-4a26-a7f3-8bb4edb10abc","trusted":false},"cell_type":"code","source":"\nprint(classification_report(test_label, pred_svc))\n\nprint(confusion_matrix(test_label, pred_svc))\n# Determine Performance Metrics\n# Calculate mean absolute percentage error (MAPE)\n\n\nError = np.mean(np.abs(np.subtract(test_label,pred_svc)))\nAverage = np.mean(test_label)\nMAPE = (Error/Average)*100\n\nprint('MAPE=',MAPE,'%.')\nprint('Accuracy=',100-MAPE,'%.')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4gJa9I6XpeaG","outputId":"6c992e25-997f-4f92-c02d-638289eaf9e3","trusted":false},"cell_type":"code","source":"# draw a bar plot with comparison of accuracy for different classification algorithms\n\n# prepare data\naccuracyVals = [ 98.21, 98.83,98.83]\nnames = ['Stochastic Gradient Descent Classifier', 'Support Vector Classifier', 'Random Forest Classifier']\n\nplt.figure(figsize=(10, 6))\nplt.bar(names, accuracyVals)\nplt.title('Classification algorithms accuracy comparison')\nplt.xlabel('classification algorithm')\nplt.ylabel('accuracy, %')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ht3U6di9qB-f","outputId":"06060705-bc89-4ea2-f861-c4d5ecc8e215","trusted":false},"cell_type":"code","source":"# draw a bar plot with comparison of accuracy for different classification algorithms\n\n# prepare data\naccuracyVals = [99.13, 97.6, 98.38]\nnames = ['Linear Regression', 'Polynomial Regression', 'Random Forest Regressor',]\n\nplt.figure(figsize=(10, 7))\nplt.bar(names, accuracyVals)\nplt.title('Regression algorithms accuracy comparison')\nplt.xlabel('Regression algorithm')\nplt.ylabel('accuracy, %')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XTPTTxeEruTJ"},"cell_type":"markdown","source":"# **What did we get?**\n\n\n\n*  **Most important features**:\n\n      1.   Total Equity\n      2.   Net borrowings\n\n\n*   **Cluster analysis**:\n\n      * 9 stock changes clusters by K-Means\n      * only 2 clusters from *fundamentals* analysis\n      \n      * Best Classifier:\n          * Random Forest Classifier\n\n*   **Best regression algorithm**\n      *   Linear regression\n\n\n*   **ANOVA**\n      *  C (Revenue) **p-value**: 1.07*e-38;\n      *  C (features) **p-value**: 2.52*e-38\n\n\n\n\n\n\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}